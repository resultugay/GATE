GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 324, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 25, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 367, in train_
    self.critic.evaluationValid(self.validation, GS, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 52, in evaluationValid
    ground_truth, pred = self.generateFullOrder(data, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 32, in generateFullOrder
    tids_detect = GS.sortGlobalOrder(attr, eid)
AttributeError: 'GlobalStructures' object has no attribute 'sortGlobalOrder'
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 201, in train
    criterion = PairWiseLossNA()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 383, in __init__
    super(PairWiseLoss, self).__init__()
TypeError: super(type, obj): obj must be an instance or subtype of type
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device) #oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device) #oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
dataset : nba
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 220, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 220, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 220, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 220, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:112: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 245, in train
    ndcg += ndcg_score([ground_truth], [pred])
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py", line 63, in inner_f
    return f(*args, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1567, in ndcg_score
    _check_dcg_target_type(y_true)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1307, in _check_dcg_target_type
    raise ValueError(
ValueError: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 245, in train
    ndcg += ndcg_score([ground_truth], [pred])
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py", line 63, in inner_f
    return f(*args, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1567, in ndcg_score
    _check_dcg_target_type(y_true)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1307, in _check_dcg_target_type
    raise ValueError(
ValueError: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 245, in train
    ndcg += ndcg_score([ground_truth], [pred])
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py", line 63, in inner_f
    return f(*args, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1567, in ndcg_score
    _check_dcg_target_type(y_true)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1307, in _check_dcg_target_type
    raise ValueError(
ValueError: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:88: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 309, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 245, in train
    ndcg += ndcg_score([ground_truth], [pred])
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/utils/validation.py", line 63, in inner_f
    return f(*args, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1567, in ndcg_score
    _check_dcg_target_type(y_true)
  File "/root/anaconda3/lib/python3.9/site-packages/sklearn/metrics/_ranking.py", line 1307, in _check_dcg_target_type
    raise ValueError(
ValueError: Only ('multilabel-indicator', 'continuous-multioutput', 'multiclass-multioutput') formats are supported. Got binary instead
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:135: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:139: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
dataset : nba
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 322, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 228, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 322, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 228, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method Critic
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 322, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 228, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
Method CreatorNA
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 58, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 322, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 228, in train
    local_batch = torch.stack(local_batch)
RuntimeError: stack expects each tensor to be equal size, but got [256, 21232] at entry 0 and [256, 11384] at entry 1
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device) #oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:98: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:95: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device) #oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:97: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:99: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
dataset : nba
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
dataset : nba
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
1. The time of evaluating training data is 9.5367431640625e-07 with 8382 number of training data
Epoch: 0 Total Loss: %.4f | execution time %.4f mins 1.9227137306166553 0.06000924507776896
Epoch: 1 Total Loss: %.4f | execution time %.4f mins 1.726926229487116 0.02639296054840088
Epoch: 2 Total Loss: %.4f | execution time %.4f mins 1.7478219099226273 0.011947822570800782
Epoch: 3 Total Loss: %.4f | execution time %.4f mins 1.7190268040639207 0.01311506430308024
Epoch: 4 Total Loss: %.4f | execution time %.4f mins 1.7466935120819522 0.013317267100016275
Epoch: 5 Total Loss: %.4f | execution time %.4f mins 1.7062176862091352 0.014520883560180664
Epoch: 6 Total Loss: %.4f | execution time %.4f mins 1.6901020770154302 0.014394394556681315
Epoch: 7 Total Loss: %.4f | execution time %.4f mins 1.690362774029147 0.012809367974599202
Epoch: 8 Total Loss: %.4f | execution time %.4f mins 1.7019122358595589 0.013668723901112874
Epoch: 9 Total Loss: %.4f | execution time %.4f mins 1.6826552042166547 0.014304816722869873
2. The time of training Creator is  24.30668807029724
3. The time of update training data is  1.2636184692382812e-05
4. The time of choosing high confidence TOs is 298.8405785560608 with 0 temporal orders
5. The time of Chase is 12.173364400863647 with 51166 temporal orders
AUG : indicator = True, the number of additional training data is 4
6. The time of adding more training data is 0.0005788803100585938
round=0 ndcg=0.9026317000389099 mrr=0.69659423828125 precision=0.44630847229925774 recall=0.6063120030291557 Fmeasure=0.5141495727113206
7. The time of evaluation is 500.91914558410645
1. The time of evaluating training data is 1.0728836059570312e-05 with 8382 number of training data
Epoch: 0 Total Loss: %.4f | execution time %.4f mins 1.7037873268105255 0.016540618737538655
Epoch: 1 Total Loss: %.4f | execution time %.4f mins 1.6738712032246619 0.013306772708892823
Epoch: 2 Total Loss: %.4f | execution time %.4f mins 1.678625549574004 0.012941261132558187
Epoch: 3 Total Loss: %.4f | execution time %.4f mins 1.657688711099786 0.01257094939549764
Epoch: 4 Total Loss: %.4f | execution time %.4f mins 1.6592203735218098 0.012828048070271809
Epoch: 5 Total Loss: %.4f | execution time %.4f mins 1.667178591286373 0.01230241854985555
Epoch: 6 Total Loss: %.4f | execution time %.4f mins 1.6846140206282185 0.012272123495737712
Epoch: 7 Total Loss: %.4f | execution time %.4f mins 1.64760895654382 0.01256999174753825
Epoch: 8 Total Loss: %.4f | execution time %.4f mins 1.6525960407181028 0.01257707675298055
Epoch: 9 Total Loss: %.4f | execution time %.4f mins 1.6490038737204082 0.012558197975158692
2. The time of training Creator is  15.44187617301941
3. The time of update training data is  7.200241088867188e-05
4. The time of choosing high confidence TOs is 364.7913341522217 with 0 temporal orders
5. The time of Chase is 0.00034880638122558594 with 0 temporal orders
AUG : indicator = True, the number of additional training data is 0
6. The time of adding more training data is 1.2159347534179688e-05
round=1 ndcg=0.9060176014900208 mrr=0.70361328125 precision=0.46124094593279485 recall=0.6281067777357062 Fmeasure=0.5318936424340075
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method CreatorNC
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:111: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
dataset : career
GATE
GATE
GATE
dataset : nba
GATE
GATE
GATE
GATE
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
mkdir: cannot create directory ‘results’: File exists
mkdir: mkdir: cannot create directory ‘results’cannot create directory ‘results’mkdir: cannot create directory ‘results’: File exists: File exists

: File exists
Method GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method GATE
Method GATE
mkdir: cannot create directory ‘results’: File exists
mkdir: cannot create directory ‘results’Method GATE
: File exists
Method GATE
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.2
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.2
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.4
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.4
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.6
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.6
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.8
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
usage: main.py [<args>] [-h | --help]
main.py: error: unrecognized arguments: --entityRatio 0.8
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Method Critic
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNC
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 383, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:134: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
./fig_a.sh: line 27: 22199 Terminated              /root/anaconda3/bin/python main.py --creator Gate --data /data/data/${data[${did}]}'/' --epoch ${epoch} --lr ${lr} --batch_size ${batch_size} --high_conf_sample_ratio ${conf_sample_size} --conf_threshold ${conf_threshold} --variant gate >> ${resultFile}
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:156: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(attribute_embeddings[k])
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
1. The time of evaluating training data is 5.9604644775390625e-06 with 8382 number of training data
Epoch: 0 Total Loss: 89.06652712403563 Accuracy: 0.526008129119873 | execution time 5.847636461257935 seconds
Epoch: 1 Total Loss: 88.88117635580143 Accuracy: 0.5613218545913696 | execution time 1.8516845703125 seconds
Epoch: 2 Total Loss: 88.81584216703239 Accuracy: 0.5812455415725708 | execution time 2.695876121520996 seconds
Epoch: 3 Total Loss: 88.7780181283535 Accuracy: 0.5903126001358032 | execution time 4.137159109115601 seconds
Epoch: 4 Total Loss: 88.70588886147313 Accuracy: 0.600811243057251 | execution time 4.857362747192383 seconds
Epoch: 5 Total Loss: 88.61006667785827 Accuracy: 0.6045096516609192 | execution time 1.77504301071167 seconds
Epoch: 6 Total Loss: 88.50647294530899 Accuracy: 0.6103554964065552 | execution time 0.4393742084503174 seconds
Epoch: 7 Total Loss: 88.38168316779225 Accuracy: 0.6170365214347839 | execution time 5.9883131980896 seconds
Epoch: 8 Total Loss: 88.24152798217838 Accuracy: 0.6156048774719238 | execution time 7.345228433609009 seconds
Epoch: 9 Total Loss: 88.15550779796209 Accuracy: 0.6191839575767517 | execution time 9.030247688293457 seconds
Epoch: 10 Total Loss: 87.96883913375225 Accuracy: 0.6271772980690002 | execution time 5.804522752761841 seconds
Epoch: 11 Total Loss: 87.92598143649013 Accuracy: 0.6311143040657043 | execution time 1.1756725311279297 seconds
Epoch: 12 Total Loss: 87.7667201352602 Accuracy: 0.6358864307403564 | execution time 8.519129037857056 seconds
Epoch: 13 Total Loss: 87.69934298145925 Accuracy: 0.630994975566864 | execution time 6.064217805862427 seconds
Epoch: 14 Total Loss: 87.66835170461512 Accuracy: 0.6346933841705322 | execution time 7.354532480239868 seconds
Epoch: 15 Total Loss: 87.5815685988417 Accuracy: 0.6416130065917969 | execution time 3.3470327854156494 seconds
Epoch: 16 Total Loss: 87.5443615894403 Accuracy: 0.6450727581977844 | execution time 3.547393560409546 seconds
Epoch: 17 Total Loss: 87.52226880551282 Accuracy: 0.6410164833068848 | execution time 4.023503065109253 seconds
Epoch: 18 Total Loss: 87.49282169827393 Accuracy: 0.6475781202316284 | execution time 11.437246561050415 seconds
Epoch: 19 Total Loss: 87.36064497032375 Accuracy: 0.6434025168418884 | execution time 6.276123046875 seconds
Epoch: 20 Total Loss: 87.33165104620235 Accuracy: 0.6487711668014526 | execution time 1.9858081340789795 seconds
Epoch: 21 Total Loss: 87.2796604534874 Accuracy: 0.6467429995536804 | execution time 3.6251022815704346 seconds
Epoch: 22 Total Loss: 87.32191216948335 Accuracy: 0.6457886099815369 | execution time 7.167543888092041 seconds
Epoch: 23 Total Loss: 87.32422364644289 Accuracy: 0.6437604427337646 | execution time 4.372625350952148 seconds
Epoch: 24 Total Loss: 87.12938934591652 Accuracy: 0.6510379314422607 | execution time 3.1520395278930664 seconds
Epoch: 25 Total Loss: 87.05714296657713 Accuracy: 0.6540205478668213 | execution time 4.872140407562256 seconds
Epoch: 26 Total Loss: 87.19273958042905 Accuracy: 0.6517537832260132 | execution time 3.752643585205078 seconds
Epoch: 27 Total Loss: 87.43762637186505 Accuracy: 0.6523502469062805 | execution time 1.8189146518707275 seconds
Epoch: 28 Total Loss: 87.02517661630309 Accuracy: 0.6527081727981567 | execution time 1.4102201461791992 seconds
Epoch: 29 Total Loss: 86.94575807473949 Accuracy: 0.6536626219749451 | execution time 10.246049880981445 seconds
2. The time of training Creator is 150.75018000602722 with 8382 number of training data 
3. The time of update training data is  1.430511474609375e-05
4. The time of choosing high confidence TOs is 2060.2472727298737 with 0 temporal orders
5. The time of Chase is 4.471962928771973 with 29969 temporal orders
AUG : indicator = True, the number of additional training data is 19876
6. The time of adding more training data is 0.7645957469940186
round=0 ndcg=0.7915074229240417 mrr=0.718414306640625 precision=0.48804502679296496 recall=0.6576448315032185 Fmeasure=0.5602917527585701 accuracy=0.6309991120447876
roundGATE=0 ndcg=0.793103039264679 mrr=0.718986451625824 precision=0.4853116876939392 recall=0.6567474441499432 Fmeasure=0.5581623606380121 accuracy=0.6283054674217896
roundFScore=0 precision=0.48770385783376446 recall=0.6567474441499432 Fmeasure=0.5597411817859103 accuracy=0.630678311152501
7. The time of evaluation is 366.6849844455719
1. The time of evaluating training data is 5.9604644775390625e-06 with 8382 number of training data
Epoch: 0 Total Loss: 295.463241356402 Accuracy: 0.6533371210098267 | execution time 0.7392458915710449 seconds
Epoch: 1 Total Loss: 292.96017489570494 Accuracy: 0.6604501605033875 | execution time 0.5422041416168213 seconds
Epoch: 2 Total Loss: 292.6330775133767 Accuracy: 0.6601316332817078 | execution time 0.8031795024871826 seconds
Epoch: 3 Total Loss: 292.3531281260927 Accuracy: 0.6644136309623718 | execution time 0.5336616039276123 seconds
Epoch: 4 Total Loss: 292.2705332092629 Accuracy: 0.6655814051628113 | execution time 0.6558215618133545 seconds
Epoch: 5 Total Loss: 292.1818398028165 Accuracy: 0.6667846441268921 | execution time 0.9324860572814941 seconds
Epoch: 6 Total Loss: 291.71986326103587 Accuracy: 0.6707127094268799 | execution time 0.5554678440093994 seconds
Epoch: 7 Total Loss: 291.3287435522252 Accuracy: 0.6715266704559326 | execution time 0.5261118412017822 seconds
Epoch: 8 Total Loss: 291.63218741288443 Accuracy: 0.6754193305969238 | execution time 0.5197367668151855 seconds
Epoch: 9 Total Loss: 290.89448482117814 Accuracy: 0.6761624813079834 | execution time 0.5122194290161133 seconds
Epoch: 10 Total Loss: 291.0288061865578 Accuracy: 0.6797367334365845 | execution time 0.5325090885162354 seconds
Epoch: 11 Total Loss: 290.5762477432275 Accuracy: 0.6797720789909363 | execution time 0.5643949508666992 seconds
Epoch: 12 Total Loss: 290.9514724690306 Accuracy: 0.679984450340271 | execution time 0.6820840835571289 seconds
Epoch: 13 Total Loss: 290.26871382990936 Accuracy: 0.6840894818305969 | execution time 0.5247941017150879 seconds
Epoch: 14 Total Loss: 290.01409603264517 Accuracy: 0.6858588457107544 | execution time 0.5810801982879639 seconds
Epoch: 15 Total Loss: 290.2903873556702 Accuracy: 0.6850449442863464 | execution time 0.5558319091796875 seconds
Epoch: 16 Total Loss: 289.45565478701565 Accuracy: 0.6879113912582397 | execution time 0.5819995403289795 seconds
Epoch: 17 Total Loss: 289.19883708749 Accuracy: 0.6920518279075623 | execution time 0.5364620685577393 seconds
Epoch: 18 Total Loss: 289.67231464885845 Accuracy: 0.6930780410766602 | execution time 0.5152668952941895 seconds
Epoch: 19 Total Loss: 288.856626443019 Accuracy: 0.693679690361023 | execution time 0.5217342376708984 seconds
Epoch: 20 Total Loss: 289.0989996819118 Accuracy: 0.6930780410766602 | execution time 0.6524722576141357 seconds
Epoch: 21 Total Loss: 288.7700540998539 Accuracy: 0.6970415711402893 | execution time 0.7685666084289551 seconds
Epoch: 22 Total Loss: 289.67299177101 Accuracy: 0.6980677843093872 | execution time 0.5242977142333984 seconds
Epoch: 23 Total Loss: 289.540411440416 Accuracy: 0.6969000101089478 | execution time 0.5768368244171143 seconds
Epoch: 24 Total Loss: 287.6474626599616 Accuracy: 0.6996248960494995 | execution time 0.6004655361175537 seconds
Epoch: 25 Total Loss: 288.1163832394048 Accuracy: 0.6998018026351929 | execution time 0.5417835712432861 seconds
Epoch: 26 Total Loss: 287.97210246962027 Accuracy: 0.7010050415992737 | execution time 0.5677564144134521 seconds
Epoch: 27 Total Loss: 287.6229416353251 Accuracy: 0.700580358505249 | execution time 0.5472304821014404 seconds
Epoch: 28 Total Loss: 287.9067271689038 Accuracy: 0.7034822106361389 | execution time 0.5318751335144043 seconds
Epoch: 29 Total Loss: 288.07649838803593 Accuracy: 0.703977644443512 | execution time 0.7331461906433105 seconds
2. The time of training Creator is 24.057795763015747 with 28258 number of training data 
3. The time of update training data is  0.004794597625732422
4. The time of choosing high confidence TOs is 114.60067248344421 with 0 temporal orders
5. The time of Chase is 0.3652818202972412 with 0 temporal orders
AUG : indicator = True, the number of additional training data is 0
6. The time of adding more training data is 0.0011038780212402344
round=1 ndcg=0.8003239631652832 mrr=0.7269083857536316 precision=0.505972163931079 recall=0.675592578568724 Fmeasure=0.5786073781974783 accuracy=0.6482181337780328
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 79, in load_data_new
    with open(self.args.data + 'training_processed.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/comm/training_processed.pkl'
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 79, in load_data_new
    with open(self.args.data + 'training_processed.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/comm/training_processed.pkl'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 79, in load_data_new
    with open(self.args.data + 'training_processed.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/comm/training_processed.pkl'
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 79, in load_data_new
    with open(self.args.data + 'training_processed.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/comm/training_processed.pkl'
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 79, in load_data_new
    with open(self.args.data + 'training_processed.pkl', 'rb') as f:
FileNotFoundError: [Errno 2] No such file or directory: '/data/data/comm/training_processed.pkl'
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Creator
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Critic
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 384, in train_
    self.creator.train(training_data, self.training, self.validation, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 218, in train
    validation_set = GateValidationTestDataset(validation_data, option)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateDataset.py", line 57, in __init__
    attribute_emb = self.data_attribute_emb[attribute]
KeyError: 'name'
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 403, in train_
    deltaTOML = self.choose_high_confidence_(self.validation, self.args.high_conf_sample_ratio, self.args.conf_threshold,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 661, in choose_high_confidence_
    diff, flagC = self.creator.predictHighConf([attribute_emb_new, val_emb_1, val_emb_2], confThreshold)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 117, in predictHighConf
    attr_emb = self.model.getAttrEmb(attr) # self.model.x_embed2last(self.model.x_input2embed(attr))
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 403, in train_
    deltaTOML = self.choose_high_confidence_(self.validation, self.args.high_conf_sample_ratio, self.args.conf_threshold,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 661, in choose_high_confidence_
    diff, flagC = self.creator.predictHighConf([attribute_emb_new, val_emb_1, val_emb_2], confThreshold)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 117, in predictHighConf
    attr_emb = self.model.getAttrEmb(attr) # self.model.x_embed2last(self.model.x_input2embed(attr))
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Method Critic
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 403, in train_
    deltaTOML = self.choose_high_confidence_(self.validation, self.args.high_conf_sample_ratio, self.args.conf_threshold,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 661, in choose_high_confidence_
    diff, flagC = self.creator.predictHighConf([attribute_emb_new, val_emb_1, val_emb_2], confThreshold)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 117, in predictHighConf
    attr_emb = self.model.getAttrEmb(attr) # self.model.x_embed2last(self.model.x_input2embed(attr))
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 403, in train_
    deltaTOML = self.choose_high_confidence_(self.validation, self.args.high_conf_sample_ratio, self.args.conf_threshold,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 661, in choose_high_confidence_
    diff, flagC = self.creator.predictHighConf([attribute_emb_new, val_emb_1, val_emb_2], confThreshold)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 117, in predictHighConf
    attr_emb = self.model.getAttrEmb(attr) # self.model.x_embed2last(self.model.x_input2embed(attr))
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 403, in train_
    deltaTOML = self.choose_high_confidence_(self.validation, self.args.high_conf_sample_ratio, self.args.conf_threshold,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 661, in choose_high_confidence_
    diff, flagC = self.creator.predictHighConf([attribute_emb_new, val_emb_1, val_emb_2], confThreshold)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 117, in predictHighConf
    attr_emb = self.model.getAttrEmb(attr) # self.model.x_embed2last(self.model.x_input2embed(attr))
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 467, in train_
    self.creator.evaluationValid(self.validation, round)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 144, in evaluationValid
    attr_emb = self.model.getAttrEmb(attributes[0])
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py", line 351, in getAttrEmb
    return nn.functional.normalize(self.c_embed2last_(self.c_input2embed(self.c_input2embed_(attr))), dim=0)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1130, in _call_impl
    return forward_call(*input, **kwargs)
  File "/root/anaconda3/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
RuntimeError: expected scalar type Float but found Double
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Critic
Method Creator
Method CreatorNC
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Critic
Method Creator
Method CreatorNC
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 159, in load_data_new
    attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
TypeError: nan_to_num(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 159, in load_data_new
    attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
TypeError: nan_to_num(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 159, in load_data_new
    attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
TypeError: nan_to_num(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 159, in load_data_new
    attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
TypeError: nan_to_num(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 65, in <module>
    gate.initialize(args)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 259, in initialize
    self.load_data_new()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 159, in load_data_new
    attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
TypeError: nan_to_num(): argument 'input' (position 1) must be Tensor, not numpy.ndarray
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method Creator
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:143: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:147: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
dataset : career
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Creator
Method Critic
Method CreatorNC
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
GATE
mkdir: cannot create directory ‘results’: File exists
Method GATE
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_layer_norm.weight', 'vocab_transform.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:116: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(oneTOEmbedds[0]).to(self.device)  # oneTOEmbedds[1], oneTOEmbedds[2]
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:118: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_pos = torch.tensor(oneTOEmbedds[1]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:120: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val_neg = torch.tensor(oneTOEmbedds[2]).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
Traceback (most recent call last):
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/main.py", line 67, in <module>
    gate.train_()
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py", line 409, in train_
    new_temporal_orders, GS, indicator = self.critic.deduce(self.ccs, deltaTOStable, deltaTOML, indicator, GS, self.args.variants)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Critic.py", line 27, in deduce
    sigma_, GS_, indicator_ = self.chase.Chase(CCs, GS, deltaTOStable, deltaTOML,
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 140, in Chase
    newH = self.envokeTOLHS(to, CCs, GS)
  File "/opt/disk1/yaoshuw/timeliness/codes/GATE/critic/Chase.py", line 37, in envokeTOLHS
    ccsIDList = ccs.getIndex()[attr]
AttributeError: 'NoneType' object has no attribute 'getIndex'
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:108: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attr = torch.tensor(attribute_emb).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  val = torch.tensor(value_emb).to(self.device)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_transform.bias', 'vocab_transform.weight', 'vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method Creator
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_layer_norm.weight', 'vocab_projector.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method Critic
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_projector.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
Method CreatorNC
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.weight', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias', 'vocab_transform.weight']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
Method CreatorNA
Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_projector.weight', 'vocab_projector.bias']
- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).
- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:159: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attribute_embeddings[k] = torch.tensor(torch.nan_to_num(attribute_embeddings[k]), dtype=torch.float32) #torch.tensor(attribute_embeddings[k])
/opt/disk1/yaoshuw/timeliness/codes/GATE/gate.py:162: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  sentence_embeddings = torch.tensor(torch.nan_to_num(sentence_embeddings), dtype=torch.float32)
/root/anaconda3/lib/python3.9/site-packages/torch/nn/functional.py:1949: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.
  warnings.warn("nn.functional.tanh is deprecated. Use torch.tanh instead.")
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:144: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[0] = torch.tensor(attributes[0]).to(self.device)
/opt/disk1/yaoshuw/timeliness/codes/GATE/creator/GateCreator.py:148: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  attributes[idx] = torch.tensor(attributes[idx]).to(self.device)
